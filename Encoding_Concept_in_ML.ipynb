{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoding Concept in ML**\n",
        "\n",
        "In machine learning (ML), **encoding** is a crucial step in preparing data for processing by machine learning algorithms. Here are some common types of encoding used in ML:"
      ],
      "metadata": {
        "id": "JZ-sTaE7tZrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Label Encoding**\n",
        "\n",
        "This is used for encoding categorical variables into numerical labels. Each unique category is assigned a numerical value. However, this encoding may not be suitable for ordinal data as it imposes an arbitrary order on the categories.\n",
        "\n",
        "Here's an example of how to perform label encoding using Python, specifically using the **LabelEncoder** class from the **scikit-learn** library:"
      ],
      "metadata": {
        "id": "ystcGLMau03a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtdvrCkabQll",
        "outputId": "292d533a-17ec-472a-f5b3-a10744150cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original categories: ['red', 'green', 'blue', 'red', 'blue']\n",
            "Encoded labels: [2 1 0 2 0]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample categorical data\n",
        "categories = ['red', 'green', 'blue', 'red', 'blue']\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the categorical data\n",
        "encoded_labels = label_encoder.fit_transform(categories)\n",
        "\n",
        "# Print the encoded labels\n",
        "print(\"Original categories:\", categories)\n",
        "print(\"Encoded labels:\", encoded_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **One-Hot Encoding**\n",
        "\n",
        "In this technique, each categorical variable is converted into a binary vector where each category is represented by a binary value (0 or 1). Each category gets its own dimension in the vector, and only one of them is \"hot\" (set to 1) while the others are \"cold\" (set to 0). One-hot encoding is suitable for categorical variables without an inherent order.\n",
        "\n",
        "In this example, we have a list of categorical data representing different types of animals. We want to perform one-hot encoding on this data.\n",
        "\n",
        "* We import the necessary libraries: OneHotEncoder from sklearn.preprocessing and numpy as np.\n",
        "\n",
        "* We define our sample categorical data in the categories list.\n",
        "\n",
        "* We initialize the OneHotEncoder object, specifying sparse=False to get a dense array as output.\n",
        "\n",
        "* Since OneHotEncoder expects a 2D array as input, we reshape the categories list into a 2D numpy array using reshape(-1, 1).\n",
        "\n",
        "* We then use the fit_transform() method of OneHotEncoder to fit the encoder to the data and transform it into one-hot encoded format.\n",
        "\n",
        "* Finally, we print both the original categories and the one-hot encoded data.\n",
        "\n",
        "The output will be a binary matrix where each row represents one observation (in this case, one animal) and each column represents a category (in this case, one animal type - cat, dog, or bird). A value of 1 indicates the presence of that category (animal type), while a value of 0 indicates its absence.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U1LJ0VsYullg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Sample categorical data\n",
        "categories = ['cat', 'dog', 'bird', 'cat', 'bird']\n",
        "\n",
        "# Initialize the OneHotEncoder\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Reshape the data to fit the expected shape for OneHotEncoder\n",
        "categories_array = np.array(categories).reshape(-1, 1)\n",
        "\n",
        "# Fit and transform the categorical data\n",
        "onehot_encoded = onehot_encoder.fit_transform(categories_array)\n",
        "\n",
        "# Print the one-hot encoded data\n",
        "print(\"Original categories:\", categories)\n",
        "print(\"One-hot encoded data:\")\n",
        "print(onehot_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAmQMX6BtxrX",
        "outputId": "e708bdfb-23ef-4936-d2ce-48129399bfd6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original categories: ['cat', 'dog', 'bird', 'cat', 'bird']\n",
            "One-hot encoded data:\n",
            "[[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ordinal Encoding**\n",
        "\n",
        "This is similar to label encoding but preserves the ordinal relationship between categories. Each category is mapped to an integer value based on its order.\n",
        "\n",
        "To perform ordinal encoding in Python, you can utilize the **OrdinalEncoder** class from scikit-learn.\n",
        "\n",
        "In this example:\n",
        "\n",
        "* We have a list of ordinal data representing different levels (e.g., 'low', 'medium', 'high').\n",
        "\n",
        "* We specify the order of categories using the category_order list.\n",
        "\n",
        "* We initialize the OrdinalEncoder object with the specified categories order.\n",
        "\n",
        "* Since OrdinalEncoder expects a 2D array as input, we reshape the categories list into a 2D list using a list comprehension.\n",
        "\n",
        "* We then use the fit_transform() method of OrdinalEncoder to fit the encoder to the data and transform it into ordinal encoded format.\n",
        "\n",
        "* Finally, we print both the original categories and the ordinal encoded data.\n",
        "\n",
        "The output will be an array of integer values representing the ordinal encoding of the categories, preserving the specified order."
      ],
      "metadata": {
        "id": "Zw8JPaXMvGjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Sample ordinal data\n",
        "categories = ['low', 'medium', 'high', 'low', 'medium', 'high']\n",
        "\n",
        "# Define the order of categories\n",
        "category_order = ['low', 'medium', 'high']\n",
        "\n",
        "# Initialize the OrdinalEncoder with the specified categories order\n",
        "ordinal_encoder = OrdinalEncoder(categories=[category_order])\n",
        "\n",
        "# Reshape the data to fit the expected shape for OrdinalEncoder\n",
        "categories_array = [[category] for category in categories]\n",
        "\n",
        "# Fit and transform the ordinal data\n",
        "ordinal_encoded = ordinal_encoder.fit_transform(categories_array)\n",
        "\n",
        "# Print the ordinal encoded data\n",
        "print(\"Original categories:\", categories)\n",
        "print(\"Ordinal encoded data:\")\n",
        "print(ordinal_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMWplViEu7rQ",
        "outputId": "aaf387af-3457-4b51-f5ea-29bf6e6969d6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original categories: ['low', 'medium', 'high', 'low', 'medium', 'high']\n",
            "Ordinal encoded data:\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [0.]\n",
            " [1.]\n",
            " [2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Binary Encoding**\n",
        "\n",
        "This technique converts categorical variables into binary representation. Each category is first converted into numeric labels, and then those labels are converted into binary digits. Each digit of the binary representation corresponds to whether the category is present (1) or absent (0).\n",
        "\n",
        "To perform binary encoding in Python, you can use libraries such as category_encoders or implement it manually. Here's how you can implement binary encoding manually.\n",
        "\n",
        "In this example:\n",
        "\n",
        "* We have a sample DataFrame df containing categorical data in the 'category' column.\n",
        "\n",
        "* We define a dictionary category_to_binary that maps each category to a binary representation. For example, category 'A' is mapped to '00', category 'B' is mapped to '01', and category 'C' is mapped to '10'.\n",
        "\n",
        "* We apply binary encoding by mapping each category to its corresponding binary representation using the map() function.\n",
        "\n",
        "* Finally, we print both the original DataFrame and the binary encoded data.\n",
        "\n",
        "The output will be a Series containing binary representations of the original categorical data. Each digit of the binary representation corresponds to whether the category is present (1) or absent (0)."
      ],
      "metadata": {
        "id": "MOzZJXQUB5JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample categorical data\n",
        "data = {'category': ['A', 'B', 'C', 'A', 'C', 'B']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create a dictionary to map categories to binary digits\n",
        "category_to_binary = {\n",
        "    'A': '00',\n",
        "    'B': '01',\n",
        "    'C': '10'\n",
        "}\n",
        "\n",
        "# Apply binary encoding to the categorical data\n",
        "binary_encoded = df['category'].map(category_to_binary)\n",
        "\n",
        "# Print the binary encoded data\n",
        "print(\"Original categories:\")\n",
        "print(df)\n",
        "print(\"\\nBinary encoded data:\")\n",
        "print(binary_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i98pKlCivQI3",
        "outputId": "97ee6a31-f356-4549-84b1-a166c34adf56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original categories:\n",
            "  category\n",
            "0        A\n",
            "1        B\n",
            "2        C\n",
            "3        A\n",
            "4        C\n",
            "5        B\n",
            "\n",
            "Binary encoded data:\n",
            "0    00\n",
            "1    01\n",
            "2    10\n",
            "3    00\n",
            "4    10\n",
            "5    01\n",
            "Name: category, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Frequency Encoding**\n",
        "\n",
        "In this method, categorical variables are replaced with the frequency of each category in the dataset. This can be useful when the frequency of occurrence of each category is informative.\n",
        "\n",
        "To perform frequency encoding in Python, you can use libraries such as Pandas. Here's how you can implement frequency encoding manually.\n",
        "\n",
        "In this example:\n",
        "\n",
        "* We have a sample DataFrame df containing categorical data in the 'category' column.\n",
        "\n",
        "* We calculate the frequency of each category using the value_counts() function with normalize=True to get the relative frequencies.\n",
        "\n",
        "* We apply frequency encoding by mapping each category to its corresponding frequency using the map() function.\n",
        "\n",
        "* Finally, we print both the original DataFrame and the frequency encoded data.\n",
        "\n",
        "The output will be a DataFrame containing the original categorical data along with a new column ('frequency_encoded') representing the frequency of each category in the dataset. This encoding method can be useful when the frequency of occurrence of each category is informative for the machine learning model.\n"
      ],
      "metadata": {
        "id": "W-5JqPIkC-sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample categorical data\n",
        "data = {'category': ['A', 'B', 'C', 'A', 'C', 'B', 'A', 'A', 'B', 'C']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate the frequency of each category\n",
        "frequency_map = df['category'].value_counts(normalize=True)\n",
        "\n",
        "# Apply frequency encoding to the categorical data\n",
        "df['frequency_encoded'] = df['category'].map(frequency_map)\n",
        "\n",
        "# Print the frequency encoded data\n",
        "print(\"Original categories:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1irwVPiBvotv",
        "outputId": "88a80863-3ee8-45ac-c48f-540eae04a68c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original categories:\n",
            "  category  frequency_encoded\n",
            "0        A                0.4\n",
            "1        B                0.3\n",
            "2        C                0.3\n",
            "3        A                0.4\n",
            "4        C                0.3\n",
            "5        B                0.3\n",
            "6        A                0.4\n",
            "7        A                0.4\n",
            "8        B                0.3\n",
            "9        C                0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Target Encoding**\n",
        "\n",
        "Target Encoding, also known as mean encoding or likelihood encoding, is a technique used in machine learning for encoding categorical variables into numerical values based on the target variable. Unlike Label Encoding or One-Hot Encoding, which do not take into account the target variable, Target Encoding utilizes information from the target variable to encode categorical features.\n",
        "\n",
        "Here's how Target Encoding works:\n",
        "\n",
        "**Calculate Mean Target for Each Category:** For each category in the categorical variable, calculate the mean of the target variable (e.g., the proportion of positive class instances).\n",
        "\n",
        "**Replace Categories with Mean Target Values:** Replace each category in the categorical variable with its corresponding mean target value.\n",
        "\n",
        "**Handle Out-of-Sample Categories:** For categories that are not present in the training data but appear in the test data, handle them appropriately (e.g., impute with the overall mean target value or a default value).\n",
        "\n",
        "Target Encoding is particularly useful for classification tasks, especially when dealing with high-cardinality categorical variables (i.e., variables with a large number of unique categories). It can provide valuable information to machine learning models, especially when there's a strong relationship between the categorical variable and the target variable.\n",
        "\n",
        "In this example:\n",
        "\n",
        "* We have a dataset with three columns: \"city\", \"population_density\" (categorical), and \"average_house_price\" (continuous target variable).\n",
        "\n",
        "* We calculate the mean target (average house price) for each population density category using groupby and mean.\n",
        "\n",
        "* Then, we replace each population density category in the \"population_density\" column with its corresponding mean target value, resulting in a new column \"encoded_population_density\".\n",
        "\n",
        "* Now, the \"encoded_population_density\" column contains the target-encoded values for each population density category based on the average house price.\n",
        "\n",
        "This information can be useful for predicting house prices based on population density using machine learning models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6O07bnW2G0S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia'],\n",
        "    'population_density': ['high', 'medium', 'high', 'low', 'medium', 'low'],\n",
        "    'average_house_price': [850000, 600000, 450000, 300000, 400000, 350000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate mean target for each population density\n",
        "mean_target = df.groupby('population_density')['average_house_price'].mean()\n",
        "\n",
        "# Replace population densities with mean target values\n",
        "df['encoded_population_density'] = df['population_density'].map(mean_target)\n",
        "\n",
        "print(\"Original data:\")\n",
        "print(df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ILfLsdMF_SZ",
        "outputId": "55546d5a-f8ce-411a-89ef-5cfe595cd5f0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data:\n",
            "           city population_density  average_house_price  \\\n",
            "0      New York               high               850000   \n",
            "1   Los Angeles             medium               600000   \n",
            "2       Chicago               high               450000   \n",
            "3       Houston                low               300000   \n",
            "4       Phoenix             medium               400000   \n",
            "5  Philadelphia                low               350000   \n",
            "\n",
            "   encoded_population_density  \n",
            "0                    650000.0  \n",
            "1                    500000.0  \n",
            "2                    650000.0  \n",
            "3                    325000.0  \n",
            "4                    500000.0  \n",
            "5                    325000.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Backward difference encoding**\n",
        "\n",
        "Backward difference encoding is a technique used for ordinal categorical variables, where each category is represented by the difference between the mean of the dependent variable for that category and the mean of the dependent variable for the previous category. This encoding method captures the relative differences between adjacent categories.\n",
        "\n",
        "Here's how backward difference encoding can be implemented in Python using pandas. In this example, we calculate the mean target for each category ('low', 'medium', 'high'), and then calculate the difference between the mean target for each category and the mean target for the previous category. We replace the categories with these backward difference values in a new column named 'backward_difference_encoded'. This column represents the backward difference-encoded ordinal categorical variable.\n",
        "\n",
        "Note that NaN is used for the first category because there is no previous category to calculate the difference from.\n",
        "\n"
      ],
      "metadata": {
        "id": "rSkBoN2ED6Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {'category': ['low', 'medium', 'high', 'low', 'medium', 'high'],\n",
        "        'target': [10, 20, 30, 15, 25, 35]}  # Sample dependent variable\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate mean target for each category\n",
        "mean_target = df.groupby('category')['target'].mean()\n",
        "\n",
        "# Calculate mean target for the previous category\n",
        "mean_target_previous = mean_target.shift(1)\n",
        "\n",
        "# Calculate difference between mean target for each category and mean target for the previous category\n",
        "backward_difference = mean_target - mean_target_previous\n",
        "\n",
        "# Replace categories with backward difference values\n",
        "df['backward_difference_encoded'] = df['category'].map(backward_difference)\n",
        "\n",
        "print(\"Original data:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c93dYsSnwlRi",
        "outputId": "b4cf5611-c0fc-4c84-e8ac-20dd54a31ca7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data:\n",
            "  category  target  backward_difference_encoded\n",
            "0      low      10                        -20.0\n",
            "1   medium      20                         10.0\n",
            "2     high      30                          NaN\n",
            "3      low      15                        -20.0\n",
            "4   medium      25                         10.0\n",
            "5     high      35                          NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embedding**\n",
        "\n",
        "Embedding is a powerful technique used in natural language processing (NLP) and recommendation systems to represent categorical variables, such as words or items, as dense vectors in a lower-dimensional continuous space. These vectors capture semantic relationships between categories, allowing similar categories to have similar vector representations. Embeddings are learned through neural network training, typically in an unsupervised or semi-supervised manner.\n",
        "\n",
        "Here's a high-level overview of how embeddings are used in NLP and recommendation systems:\n",
        "\n",
        "**Word Embeddings in NLP:** In NLP tasks such as sentiment analysis, machine translation, or named entity recognition, words are represented as dense vectors in an embedding space. Word embeddings capture semantic relationships between words. Words with similar meanings or contexts are mapped to nearby points in the embedding space. Popular word embedding techniques include Word2Vec, GloVe (Global Vectors for Word Representation), and FastText.\n",
        "These embeddings are often pre-trained on large text corpora and then fine-tuned on specific tasks.\n",
        "\n",
        "**Item Embeddings in Recommendation Systems:**In recommendation systems, items (e.g., movies, products) are represented as dense vectors in an embedding space.\n",
        "Item embeddings capture latent features of items and user preferences. Similar items are mapped to nearby points in the embedding space.\n",
        "Embeddings can be learned from user-item interaction data using techniques such as matrix factorization, neural collaborative filtering, or deep learning-based models.These embeddings are used to generate recommendations by finding similar items based on their vector representations.\n",
        "\n",
        "Here's a simplified example of how to use embeddings for word representation in Python using TensorFlow/Keras. In this example, we define an embedding layer with a vocabulary size of 10,000 and an embedding dimension of 100. We then apply the embedding layer to input data consisting of word indices. The output is the embedded representation of the input data, where each word index is mapped to a dense vector in the embedding space."
      ],
      "metadata": {
        "id": "Cq0U8mDnFNwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Example vocabulary\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "\n",
        "# Define the embedding layer\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "\n",
        "# Example input data (word indices)\n",
        "input_data = tf.constant([[1, 5, 8], [2, 7, 0]])\n",
        "\n",
        "# Apply the embedding layer to the input data\n",
        "embedded_data = embedding_layer(input_data)\n",
        "\n",
        "# Print the embedded data\n",
        "print(embedded_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdBv4BfTv4JE",
        "outputId": "cacfe465-a5de-4d6a-dc32-54e7831ef1ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[ 0.03909    -0.02228323 -0.02981289  0.0449113   0.03844407\n",
            "    0.00491518  0.04882425 -0.03801579  0.02137852 -0.02503163\n",
            "   -0.00622847  0.03244979 -0.02821611  0.00798281  0.01105924\n",
            "    0.020074    0.04368163  0.02184993 -0.00857129 -0.00340353\n",
            "   -0.04006969  0.01594497  0.04521877  0.01394308 -0.00557312\n",
            "    0.04327646 -0.01530663 -0.00655278  0.03958556  0.02479544\n",
            "   -0.04309788  0.03645028  0.04386966  0.00230924  0.00407517\n",
            "   -0.00351539  0.02980191  0.01750701  0.04364634  0.02358819\n",
            "   -0.04524968 -0.02400434 -0.01403198 -0.00854195 -0.02708879\n",
            "   -0.00536134 -0.03376164  0.02267481 -0.04519984  0.04025909\n",
            "   -0.00030692  0.0216408  -0.03237735  0.0287937  -0.01087565\n",
            "    0.00432619  0.0060644   0.02265669  0.02737783  0.03406625\n",
            "    0.02862075 -0.00952833 -0.04593113 -0.04478073  0.01866552\n",
            "    0.04952565  0.0339929  -0.00795019  0.03934612  0.00547834\n",
            "   -0.03424891  0.04806289  0.03041938  0.02892997  0.01106149\n",
            "    0.01061774 -0.01199242 -0.00492664  0.04221424 -0.0243047\n",
            "   -0.0086216  -0.030652    0.00417402  0.04247483  0.04362548\n",
            "   -0.02121148 -0.00964053  0.0255505  -0.00697631  0.03510045\n",
            "    0.02410654  0.04727281 -0.00645782  0.0156819  -0.02598777\n",
            "   -0.01561425 -0.00934831 -0.00369936 -0.03413703 -0.00114816]\n",
            "  [-0.0163349  -0.01422241  0.03501889  0.0371557   0.00379641\n",
            "    0.02723727  0.0412262   0.0281386   0.02566497 -0.02800503\n",
            "    0.00555662  0.03990698 -0.04150865 -0.0322693   0.01876729\n",
            "    0.04360617  0.00462509 -0.0201501   0.036549   -0.01509013\n",
            "   -0.00201621 -0.02808627 -0.02415756  0.02185723 -0.00733051\n",
            "    0.00243372 -0.04728769 -0.0465529   0.00055257 -0.03117181\n",
            "    0.00519877  0.04526697  0.04618699 -0.04692681 -0.04237334\n",
            "    0.01411691 -0.02836348 -0.02156078  0.0453714   0.02185841\n",
            "   -0.01220256 -0.01671957 -0.04150715 -0.00141196  0.00728529\n",
            "   -0.02886379  0.03962543  0.03278602  0.04730291 -0.02720797\n",
            "   -0.02605972 -0.03719366 -0.00748265  0.00430353 -0.0184175\n",
            "   -0.02108148 -0.00369285 -0.02968904 -0.006721    0.01726921\n",
            "   -0.00366741  0.04104782 -0.01168212  0.04791996  0.01956173\n",
            "   -0.0187497   0.04827919 -0.03606645 -0.00803924  0.00897775\n",
            "    0.01387857  0.046574    0.00900964  0.04655478  0.0150413\n",
            "    0.01392687 -0.04555599  0.00365325 -0.01048638  0.01577735\n",
            "   -0.0201407   0.03171236 -0.01595366  0.04503096 -0.00592574\n",
            "   -0.0333262  -0.03224206 -0.03973501  0.00519169 -0.02954274\n",
            "   -0.02651448  0.04395938  0.01584878 -0.03934998 -0.03370355\n",
            "   -0.01153884  0.00994373 -0.00609861  0.04704081  0.01459331]\n",
            "  [ 0.01476702 -0.04532907 -0.02410823 -0.03807372 -0.04709498\n",
            "   -0.02570963 -0.00935692 -0.01568791  0.0176642   0.02384086\n",
            "   -0.04953888  0.04040701  0.03554532 -0.02784777  0.01209779\n",
            "   -0.04217105 -0.04750304 -0.03238003 -0.04821699  0.03247695\n",
            "    0.03834694 -0.0414312   0.04544418  0.0104847   0.00220988\n",
            "    0.02850885  0.02744268 -0.00741893  0.00455626 -0.02564004\n",
            "    0.01178267  0.04513253  0.00934937 -0.04609934  0.03113899\n",
            "   -0.02969652 -0.03840991 -0.00884908  0.02214034 -0.04655187\n",
            "    0.03982267  0.03737896 -0.04490972  0.02217359 -0.02726916\n",
            "    0.01129296  0.04439367  0.0094719   0.00807959  0.02233689\n",
            "    0.00412682 -0.04704842 -0.02153391 -0.01931144  0.01116465\n",
            "   -0.0349767  -0.03566613 -0.01949228  0.00325863  0.00234751\n",
            "   -0.00022714 -0.04431519  0.01034142  0.02256748  0.04057509\n",
            "   -0.03728823  0.00697063 -0.00827403  0.01603654 -0.02380532\n",
            "   -0.0396103  -0.03849638 -0.03138628 -0.01090515 -0.04024578\n",
            "   -0.02358617 -0.0296106   0.03110112  0.02102533 -0.00585973\n",
            "   -0.01837087 -0.0457144  -0.00960981  0.00079256  0.04837218\n",
            "    0.00579488 -0.00457    -0.04869146  0.0050518  -0.00011403\n",
            "   -0.00798179 -0.03508108  0.04941286  0.00997379  0.02662246\n",
            "   -0.00959362  0.02378013 -0.0236686   0.01634273 -0.00418382]]\n",
            "\n",
            " [[-0.04614003  0.00650358  0.04128513  0.02827458 -0.04212291\n",
            "    0.03873177 -0.03429906 -0.00638785  0.02702254 -0.00100677\n",
            "    0.01487644 -0.04089128 -0.02415326 -0.01941763 -0.02268546\n",
            "   -0.02519943  0.00467402 -0.01795014  0.02736929  0.02069939\n",
            "   -0.0415209   0.04423645 -0.00330942 -0.00310351 -0.01843052\n",
            "   -0.02912576 -0.0231254   0.0229652  -0.03901627 -0.01774905\n",
            "   -0.04824152 -0.00702876  0.0122183   0.04279444  0.04946065\n",
            "   -0.00664799  0.03938259 -0.01024053 -0.02516285  0.0476449\n",
            "    0.00460038 -0.03826277  0.00139456  0.03472928 -0.02208468\n",
            "   -0.03432484  0.01006402  0.03974915 -0.02044252  0.02862943\n",
            "    0.0156112   0.0279413   0.02564697  0.00503366  0.02161226\n",
            "    0.04546681 -0.03214156  0.03892635  0.00339826 -0.04933054\n",
            "   -0.00170441  0.04501123  0.00489925  0.00963863  0.02857635\n",
            "   -0.04169041  0.04847911 -0.01314015  0.04143163 -0.03047268\n",
            "    0.02614099 -0.00664786  0.01709751 -0.02049441  0.02037625\n",
            "    0.03847687  0.0295236  -0.03182789 -0.03242897 -0.04447012\n",
            "   -0.04765077 -0.039947    0.03239096 -0.04453961 -0.01500614\n",
            "   -0.01373269  0.00799205 -0.01157682 -0.02655658  0.00251025\n",
            "    0.01412549 -0.00239583 -0.03791127  0.01368317  0.01482425\n",
            "    0.03865707 -0.03908918 -0.02020924  0.01172281 -0.04065536]\n",
            "  [-0.0328967   0.01770654  0.01540763 -0.02762132  0.0323371\n",
            "    0.01994983 -0.01791688 -0.01503615  0.01961478  0.03669156\n",
            "   -0.01305798  0.03735724  0.03628493  0.00915573  0.03012183\n",
            "   -0.04168209  0.03080137  0.03941066 -0.04435289  0.02749021\n",
            "   -0.02705392 -0.04675717  0.03695798  0.04747151  0.01109169\n",
            "   -0.0016109   0.00935717  0.01647537  0.03096715 -0.02229849\n",
            "    0.02743426 -0.03521501 -0.02991735  0.00879165  0.03880287\n",
            "   -0.04743756 -0.0357368   0.03053906 -0.0392488  -0.00242814\n",
            "    0.04433726  0.01892889 -0.02172849 -0.02249416  0.00689125\n",
            "    0.034607    0.04487536 -0.01481419 -0.01958361 -0.02494137\n",
            "    0.04138393  0.03535904 -0.04840502 -0.0362283   0.02133865\n",
            "    0.00756256 -0.0376991   0.01652285  0.04706918  0.04373373\n",
            "   -0.01530652  0.03680196  0.01524618 -0.02324388 -0.0006396\n",
            "   -0.01829468  0.04436367 -0.00558169 -0.00627174 -0.04330975\n",
            "   -0.04391735  0.04006349 -0.02915972  0.04695195 -0.00330428\n",
            "   -0.01750436  0.01032454  0.04379208  0.03485474 -0.04280872\n",
            "    0.01620821  0.0397935   0.04193821  0.04857821  0.02586168\n",
            "   -0.03871344 -0.04743737  0.02235056  0.00535854 -0.01145105\n",
            "   -0.04798413  0.03963982  0.04501608 -0.00832453 -0.01700085\n",
            "   -0.03986097  0.03725343  0.01681919  0.01490607  0.03738082]\n",
            "  [-0.00943993 -0.02415382  0.04976596 -0.00864618  0.00301705\n",
            "   -0.04906181  0.00955148  0.00398775  0.04577706 -0.02439219\n",
            "    0.03763593 -0.03638741 -0.01715064  0.03158014  0.02752728\n",
            "    0.02876027  0.02406769 -0.04731754 -0.00308152  0.00757811\n",
            "   -0.0064338   0.04106033  0.03885913  0.04296173 -0.02654097\n",
            "   -0.04480056  0.01016481 -0.01698786 -0.04070122 -0.00481732\n",
            "   -0.03749443  0.01626423 -0.03151596  0.00016291 -0.03801375\n",
            "    0.02319307 -0.01978798  0.03536561 -0.00850993 -0.03582261\n",
            "   -0.00125434 -0.0082603   0.04777339 -0.02318649  0.00931956\n",
            "    0.01747921 -0.02963226  0.02576074  0.04525824 -0.01142914\n",
            "   -0.03525592  0.03534973 -0.01104261 -0.0165009  -0.03720871\n",
            "    0.01602164 -0.01090457 -0.01634103 -0.04295943  0.03276518\n",
            "   -0.00927537  0.03166077  0.01436672  0.02972719 -0.00520172\n",
            "    0.01175498 -0.00172702 -0.02717694 -0.01304067 -0.0323536\n",
            "    0.027018   -0.02403896 -0.02804126  0.01168042 -0.02491035\n",
            "    0.00436408  0.0314875  -0.029177    0.00368544  0.00334299\n",
            "    0.02014131 -0.00698855 -0.00882735  0.02408535 -0.02507607\n",
            "   -0.04148747 -0.01515383  0.01588603  0.01483038  0.00305552\n",
            "   -0.02339064  0.04570676  0.00170957 -0.00013871 -0.03292928\n",
            "    0.02059958  0.01821792  0.01566583  0.01638422  0.04399078]]], shape=(2, 3, 100), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}